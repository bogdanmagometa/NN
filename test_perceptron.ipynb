{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nn.nn import Value\n",
    "from nn.models import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_perceptron_value(data: List[Value], weights: List[Value], bias: Value) -> Value:\n",
    "    \"\"\"Create and return a perceptron\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list of Value objects (len should be >= 1)\n",
    "        each entry represents a feature of x\n",
    "    weights: list of Value objects (len should be >= 1)\n",
    "        list of weights\n",
    "    bias: Value\n",
    "        represents a bias term\n",
    "    \"\"\"\n",
    "    summands = map(lambda d, w: d * w, data, weights)\n",
    "    \n",
    "    linear_comb = next(summands)\n",
    "    for summand in summands:\n",
    "        linear_comb = linear_comb + summand\n",
    "\n",
    "    logit = linear_comb + bias\n",
    "    \n",
    "    probability = logit.sigmoid()\n",
    "\n",
    "    return probability\n",
    "\n",
    "def make_binary_crossentropy_loss(prediction_values: List[Value], ground_truth: List[float]):\n",
    "    loss = 0\n",
    "    for p, g in zip(prediction_values, ground_truth):\n",
    "        if np.isclose(g, 0):\n",
    "            example_cost = (1 - p).log()\n",
    "        elif np.isclose(g, 1):\n",
    "            example_cost = p.log()\n",
    "        loss = loss + example_cost\n",
    "    loss = (-1) * loss\n",
    "    return loss\n",
    "#TODO: write log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example that creates loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [Value(0), Value(1)]\n",
    "bias = Value(2)\n",
    "\n",
    "data1 = [Value(3), Value(4)]\n",
    "perceptron1 = make_perceptron_value(data1, weights, bias)\n",
    "\n",
    "data2 = [Value(0), Value(0)]\n",
    "perceptron2 = make_perceptron_value(data2, weights, bias)\n",
    "\n",
    "loss = make_binary_crossentropy_loss([perceptron1, perceptron2], [1, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to train a perceptron to behave like an AND gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Loss = 0.8523001622353087\n",
      "y_pred = 0.2975929795474892 0.215075028131512 0.40254539200658424 0.3035001821013969\n",
      "Iteration 1\n",
      "Loss = 0.7819736381602448\n",
      "y_pred = 0.27788458503673213 0.23704522675989487 0.4036627758292637 0.35338448797326316\n",
      "Iteration 2\n",
      "Loss = 0.7144655391144722\n",
      "y_pred = 0.25901885709056516 0.25821321277798914 0.40488487928699657 0.40387282607701275\n",
      "Iteration 3\n",
      "Loss = 0.6530642045462869\n",
      "y_pred = 0.23912437719267698 0.27417204218395824 0.4014003348186365 0.4462819000905386\n",
      "Iteration 4\n",
      "Loss = 0.6000767394429656\n",
      "y_pred = 0.21876908859016816 0.28406564084853114 0.3931158181405811 0.47857241765076836\n",
      "Iteration 5\n",
      "Loss = 0.5549799725401626\n",
      "y_pred = 0.19932534308570896 0.28959482274208664 0.3824323426069891 0.5034816296850491\n",
      "Iteration 6\n",
      "Loss = 0.5163807424873403\n",
      "y_pred = 0.1815780707775113 0.29246584141306775 0.3712612555775008 0.5238430185171598\n",
      "Iteration 7\n",
      "Loss = 0.48306725508856885\n",
      "y_pred = 0.1656899416314183 0.29364732849156266 0.3604546020294887 0.5412460898996734\n",
      "Iteration 8\n",
      "Loss = 0.45406458044463016\n",
      "y_pred = 0.15155140346355642 0.29364784326011917 0.35029053348111405 0.5565049865405309\n",
      "Iteration 9\n",
      "Loss = 0.4285881685529913\n",
      "y_pred = 0.13897822572972487 0.2927736029459263 0.34082768542376934 0.5700961271608975\n",
      "Iteration 10\n",
      "Loss = 0.4060121808423044\n",
      "y_pred = 0.12778110667684597 0.29123678109834955 0.3320465331907689 0.5823416236362913\n",
      "Iteration 11\n",
      "Loss = 0.3858402934388652\n",
      "y_pred = 0.11778627774650043 0.2891957887645411 0.32389940163541747 0.5934776322314773\n",
      "Iteration 12\n",
      "Loss = 0.3676790693404283\n",
      "y_pred = 0.1088404507678797 0.2867728946652969 0.3163297444750363 0.6036843742547802\n",
      "Iteration 13\n",
      "Loss = 0.3512155192875356\n",
      "y_pred = 0.10081065965454082 0.2840639506004315 0.30928048666265173 0.6131026180830815\n",
      "Iteration 14\n",
      "Loss = 0.3361991206951606\n",
      "y_pred = 0.09358240330341329 0.2811446945472499 0.30269777343873994 0.6218441438223637\n",
      "Iteration 15\n",
      "Loss = 0.32242779908632097\n",
      "y_pred = 0.08705733220527342 0.2780752104512902 0.29653252777074457 0.6299989070430669\n",
      "Iteration 16\n",
      "Loss = 0.3097371505736611\n",
      "y_pred = 0.08115096663114622 0.2749032409356509 0.290740903072611 0.637640148774055\n",
      "Iteration 17\n",
      "Loss = 0.2979922057620603\n",
      "y_pred = 0.07579063062713524 0.27166672018382837 0.28528417949003715 0.6448281358424174\n",
      "Iteration 18\n",
      "Loss = 0.2870811427027734\n",
      "y_pred = 0.07091365510591265 0.26839574604832533 0.28012839474869977 0.6516129490136127\n",
      "Iteration 19\n",
      "Loss = 0.27691047781679246\n",
      "y_pred = 0.06646584696237422 0.2651141337839768 0.275243866626651 0.6580365906910669\n",
      "Iteration 20\n",
      "Loss = 0.26740137216623266\n",
      "y_pred = 0.06240019876906814 0.26184064991622275 0.2706046911792954 0.6641345967060691\n",
      "Iteration 21\n",
      "Loss = 0.2584867787730058\n",
      "y_pred = 0.05867580686078356 0.25858999751495765 0.26618826022514097 0.6699372812189148\n",
      "Iteration 22\n",
      "Loss = 0.2501092253363989\n",
      "y_pred = 0.055256965841307674 0.2553736060971898 0.2619748188050382 0.6754707068844144\n",
      "Iteration 23\n",
      "Loss = 0.24221907874310442\n",
      "y_pred = 0.052112410605746125 0.25220026679202306 0.25794707061158884 0.6807574471794482\n",
      "Iteration 24\n",
      "Loss = 0.2347731766775923\n",
      "y_pred = 0.04921468091454468 0.24907664426862378 0.2540898324501246 0.6858171900873566\n",
      "Iteration 25\n",
      "Loss = 0.2277337405367756\n",
      "y_pred = 0.04653958749130871 0.24600769010995496 0.2503897351903852 0.690667219704851\n",
      "Iteration 26\n",
      "Loss = 0.22106750525093494\n",
      "y_pred = 0.04406576217753991 0.2429969771197554 0.2468349669805743 0.6953228032014965\n",
      "Iteration 27\n",
      "Loss = 0.21474501745892166\n",
      "y_pred = 0.04177427774600879 0.24004697003334324 0.24341505389224025 0.699797503877674\n",
      "Iteration 28\n",
      "Loss = 0.20874006524282684\n",
      "y_pred = 0.03964832554879799 0.23715924496483518 0.24012067315664332 0.7041034361290889\n",
      "Iteration 29\n",
      "Loss = 0.20302921137892588\n",
      "y_pred = 0.03767294130275446 0.23433466745376583 0.2369434944537782 0.7082514744472667\n",
      "Iteration 30\n",
      "Loss = 0.19759140860322932\n",
      "y_pred = 0.035834771056680335 0.23157353701992203 0.23387604515685165 0.7122514258241363\n",
      "Iteration 31\n",
      "Loss = 0.19240768030305416\n",
      "y_pred = 0.03412187080415712 0.22887570458252934 0.23091159592237268 0.716112172841559\n",
      "Iteration 32\n",
      "Loss = 0.1874608537554828\n",
      "y_pred = 0.03252353436063347 0.22624066786226862 0.22804406349658546 0.7198417931386573\n",
      "Iteration 33\n",
      "Loss = 0.18273533584987026\n",
      "y_pred = 0.031030145062472404 0.22366764889543514 0.22526792805579415 0.723447659734166\n",
      "Iteration 34\n",
      "Loss = 0.1782169233820698\n",
      "y_pred = 0.029633047610044854 0.22115565699720224 0.22257816279915577 0.7269365257449477\n",
      "Iteration 35\n",
      "Loss = 0.17389264165981044\n",
      "y_pred = 0.028324437000263412 0.21870353987498692 0.21997017386433815 0.730314596316885\n",
      "Iteration 36\n",
      "Loss = 0.16975060643486375\n",
      "y_pred = 0.027097262003397633 0.216310025081531 0.21743974894033125 0.7335875900198865\n",
      "Iteration 37\n",
      "Loss = 0.1657799051694997\n",
      "y_pred = 0.025945141056529872 0.2139737535853899 0.21498301321134652 0.7367607915168217\n",
      "Iteration 38\n",
      "Loss = 0.16197049442014458\n",
      "y_pred = 0.024862288789204025 0.21169330690417992 0.21259639148587559 0.7398390969684677\n",
      "Iteration 39\n",
      "Loss = 0.15831311073086313\n",
      "y_pred = 0.02384345167980078 0.20946722897735573 0.21027657555056736 0.7428270533615039\n",
      "Iteration 40\n",
      "Loss = 0.1547991929114592\n",
      "y_pred = 0.02288385157525642 0.20729404373790977 0.20802049594442804 0.7457288927279959\n",
      "Iteration 41\n",
      "Loss = 0.1514208139584296\n",
      "y_pred = 0.021979136001005584 0.20517226916616954 0.20582529747934528 0.7485485620502225\n",
      "Iteration 42\n",
      "Loss = 0.14817062118361762\n",
      "y_pred = 0.02112533434976917 0.20310042846580784 0.2036883179420057 0.751289749504615\n",
      "Iteration 43\n",
      "Loss = 0.145041783361905\n",
      "y_pred = 0.02031881917290263 0.2010770588858569 0.20160706950330978 0.7539559075856557\n",
      "Iteration 44\n",
      "Loss = 0.14202794390847825\n",
      "y_pred = 0.019556271911215388 0.19910071861780768 0.19957922243734497 0.7565502735591482\n",
      "Iteration 45\n",
      "Loss = 0.13912317925801362\n",
      "y_pred = 0.01883465249731029 0.1971699921196389 0.19760259081531556 0.7590758876199231\n",
      "Iteration 46\n",
      "Loss = 0.1363219617502059\n",
      "y_pred = 0.018151172341695134 0.19528349415555368 0.19567511989267322 0.7615356090683026\n",
      "Iteration 47\n",
      "Loss = 0.1336191264344175\n",
      "y_pred = 0.017503270282728946 0.19343987278862998 0.19379487495179268 0.763932130769854\n",
      "Iteration 48\n",
      "Loss = 0.1310098412955139\n",
      "y_pred = 0.016888591137960988 0.1916378115213679 0.19196003139937187 0.7662679921219332\n",
      "Iteration 49\n",
      "Loss = 0.12848958047688505\n",
      "y_pred = 0.01630496654330384 0.18987603074449425 0.19016886594852844 0.768545590716622\n",
      "Iteration 50\n",
      "Loss = 0.12605410013812923\n",
      "y_pred = 0.01575039780815416 0.1881532886259711 0.18841974874134018 0.7707671928615035\n",
      "Iteration 51\n",
      "Loss = 0.123699416636228\n",
      "y_pred = 0.015223040550190027 0.18646838154879897 0.1867111362891765 0.7729349430962775\n",
      "Iteration 52\n",
      "Loss = 0.12142178676210735\n",
      "y_pred = 0.014721190904088716 0.18482014418699008 0.18504156512629388 0.7750508728235946\n",
      "Iteration 53\n",
      "Loss = 0.11921768980074948\n",
      "y_pred = 0.014243273124616271 0.18320744929326352 0.18340964608741225 0.7771169081560322\n",
      "Iteration 54\n",
      "Loss = 0.11708381121368255\n",
      "y_pred = 0.013787828427099934 0.1816292072589664 0.18181405913281598 0.7791348770672618\n",
      "Iteration 55\n",
      "Loss = 0.11501702776870038\n",
      "y_pred = 0.013353504927757769 0.18008436549595788 0.1802535486553484 0.7811065159237294\n",
      "Iteration 56\n",
      "Loss = 0.11301439396383199\n",
      "y_pred = 0.012939048563188355 0.178571907681308 0.1787269192128183 0.7830334754632338\n",
      "Iteration 57\n",
      "Loss = 0.11107312961153061\n",
      "y_pred = 0.012543294882902857 0.17709085289831072 0.17723303163707743 0.7849173262783108\n",
      "Iteration 58\n",
      "Loss = 0.1091906084653105\n",
      "y_pred = 0.012165161621439756 0.17564025470124248 0.17577079947760507 0.7867595638551129\n",
      "Iteration 59\n",
      "Loss = 0.10736434778505119\n",
      "y_pred = 0.011803641967612284 0.17421920012626807 0.17433918574301968 0.7885616132122727\n",
      "Iteration 60\n",
      "Loss = 0.10559199874927191\n",
      "y_pred = 0.011457798458033623 0.1728268086667416 0.17293719990870113 0.7903248331789192\n",
      "Iteration 61\n",
      "Loss = 0.10387133763314554\n",
      "y_pred = 0.011126757430442103 0.17146223122770948 0.1715638951627731 0.792050520346433\n",
      "Iteration 62\n",
      "Loss = 0.10220025768011269\n",
      "y_pred = 0.010809703979674862 0.17012464907157274 0.17021836586617498 0.7937399127245478\n",
      "Iteration 63\n",
      "Loss = 0.10057676160288213\n",
      "y_pred = 0.010505877365557529 0.1688132727645129 0.1688997452055373 0.7953941931289767\n",
      "Iteration 64\n",
      "Loss = 0.09899895465652386\n",
      "y_pred = 0.010214566827610252 0.1675273411313368 0.16760720302014562 0.7970144923247445\n",
      "Iteration 65\n",
      "Loss = 0.09746503823242852\n",
      "y_pred = 0.00993510776642158 0.16626612022478632 0.1663399437864871 0.7986018919467966\n",
      "Iteration 66\n",
      "Loss = 0.09597330392723497\n",
      "y_pred = 0.00966687825590025 0.16502890231402834 0.1650972047457882 0.8001574272171803\n",
      "Iteration 67\n",
      "Loss = 0.09452212804551681\n",
      "y_pred = 0.009409295854458126 0.16381500489594686 0.16387825416160828 0.8016820894760837\n",
      "Iteration 68\n",
      "Loss = 0.09310996649916459\n",
      "y_pred = 0.009161814686570742 0.16262376973195536 0.16268238969598983 0.8031768285422671\n",
      "Iteration 69\n",
      "Loss = 0.09173535007006112\n",
      "y_pred = 0.008923922769162478 0.16145456191230856 0.1615089368939219 0.8046425549168681\n",
      "Iteration 70\n",
      "Loss = 0.09039688000589921\n",
      "y_pred = 0.008695139559920437 0.16030676894928433 0.16035724776696003 0.8060801418431927\n",
      "Iteration 71\n",
      "Loss = 0.08909322392187621\n",
      "y_pred = 0.008475013706997018 0.15917979990011252 0.15922669946780565 0.8074904272338892\n",
      "Iteration 72\n",
      "Loss = 0.08782311198357506\n",
      "y_pred = 0.008263120981653108 0.15807308452012603 0.1581166930484845 0.8088742154758355\n",
      "Iteration 73\n",
      "Loss = 0.08658533334863038\n",
      "y_pred = 0.008059062377253443 0.1569860724462807 0.1570266522954979 0.8102322791220951\n",
      "Iteration 74\n",
      "Loss = 0.08537873284683499\n",
      "y_pred = 0.007862462359681202 0.1559182324109332 0.155956022635973 0.8115653604794628\n",
      "Iteration 75\n",
      "Loss = 0.0842022078801741\n",
      "y_pred = 0.007672967255714455 0.15486905148555602 0.15490427010940963 0.8128741730993467\n",
      "Iteration 76\n",
      "Loss = 0.08305470552592525\n",
      "y_pred = 0.007490243767223455 0.15383803435390334 0.15387088040012697 0.8141594031790478\n",
      "Iteration 77\n",
      "Loss = 0.08193521982744306\n",
      "y_pred = 0.007313977600224351 0.15282470261401676 0.15285535792596844 0.8154217108799002\n",
      "Iteration 78\n",
      "Loss = 0.08084278925857831\n",
      "y_pred = 0.007143872198876706 0.1518285941083608 0.15185722497922008 0.8166617315681681\n",
      "Iteration 79\n",
      "Loss = 0.07977649434888716\n",
      "y_pred = 0.006979647575454428 0.1508492622813049 0.15087602091605626 0.8178800769841116\n",
      "Iteration 80\n",
      "Loss = 0.07873545545786684\n",
      "y_pred = 0.006821039228164183 0.14988627556311812 0.1499113013911461 0.8190773363441788\n",
      "Iteration 81\n",
      "Loss = 0.0777188306874396\n",
      "y_pred = 0.006667797139443184 0.1489392167796076 0.14896263763434103 0.820254077380884\n",
      "Iteration 82\n",
      "Loss = 0.07672581392279296\n",
      "y_pred = 0.006519684848049147 0.14800768258650918 0.14802961576661894 0.821410847324561\n",
      "Iteration 83\n",
      "Loss = 0.07575563299249082\n",
      "y_pred = 0.006376478588867324 0.14709128292772866 0.14711183615269244 0.8225481738308535\n",
      "Iteration 84\n",
      "Loss = 0.074807547939501\n",
      "y_pred = 0.006237966494910786 0.14618964051653285 0.14620891278789763 0.8236665658574966\n",
      "Iteration 85\n",
      "Loss = 0.07388084939545236\n",
      "y_pred = 0.006103947856486654 0.1453023903387922 0.14532047271716664 0.8247665144936801\n",
      "Iteration 86\n",
      "Loss = 0.07297485705103583\n",
      "y_pred = 0.00597423243294904 0.1444291791773933 0.14444615548405879 0.8258484937450259\n",
      "Iteration 87\n",
      "Loss = 0.07208891821601965\n",
      "y_pred = 0.005848639812863677 0.14356966515695 0.14358561260797792 0.8269129612769872\n",
      "Iteration 88\n",
      "Loss = 0.07122240646284939\n",
      "y_pred = 0.005726998818774957 0.14272351730796778 0.14273850708784636 0.8279603591192733\n",
      "Iteration 89\n",
      "Loss = 0.07037472034826459\n",
      "y_pred = 0.005609146953096448 0.14189041514963496 0.14190451293063075 0.8289911143337086\n",
      "Iteration 90\n",
      "Loss = 0.06954528220778344\n",
      "y_pred = 0.005494929881945317 0.14107004829043918 0.14108331470323357 0.8300056396477622\n",
      "Iteration 91\n",
      "Loss = 0.06873353701829246\n",
      "y_pred = 0.005384200954012358 0.1402621160458346 0.14027460710636924 0.8310043340558295\n",
      "Iteration 92\n",
      "Loss = 0.06793895132432828\n",
      "y_pred = 0.005276820751805165 0.13946632707221207 0.1394780945691421 0.8319875833901935\n",
      "Iteration 93\n",
      "Loss = 0.06716101222396563\n",
      "y_pred = 0.005172656672825308 0.1386823990164499 0.13869349086313018 0.8329557608634651\n",
      "Iteration 94\n",
      "Loss = 0.06639922641051778\n",
      "y_pred = 0.005071582538443226 0.1379100581803529 0.13792051873486438 0.833909227584175\n",
      "Iteration 95\n",
      "Loss = 0.06565311926653203\n",
      "y_pred = 0.004973478228418911 0.1371490391993128 0.13715890955566343 0.8348483330470776\n",
      "Iteration 96\n",
      "Loss = 0.06492223400681085\n",
      "y_pred = 0.004878229339184331 0.13639908473455217 0.1364084029878575 0.835773415599622\n",
      "Iteration 97\n",
      "Loss = 0.06420613086742143\n",
      "y_pred = 0.004785726864156359 0.13565994517833882 0.13566874666649534 0.836684802885947\n",
      "Iteration 98\n",
      "Loss = 0.06350438633786787\n",
      "y_pred = 0.0046958668944882395 0.13493137837158445 0.1349396958956875 0.8375828122696687\n",
      "Iteration 99\n",
      "Loss = 0.06281659243379674\n",
      "y_pred = 0.0046085503387947434 0.13421314933326814 0.13422101335879585 0.8384677512366483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bohdan/MFML/NN/nn/nn.py:77: RuntimeWarning: invalid value encountered in log\n",
      "  local_grad_wrt_exponent = np.log(self.data) * out.data\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    ]\n",
    "\n",
    "Y = [0, 0, 0, 1]\n",
    "\n",
    "perceptron = Perceptron(no_weights=2)\n",
    "\n",
    "def simple_gradient_descent_iteration(model, X, Y, alpha=1e-2):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.grad = 0\n",
    "\n",
    "    loss = sum((model(x) - y)**2 for x, y in zip(X, Y))\n",
    "\n",
    "    Value.back_prop(loss)\n",
    "    print(\"Loss =\", loss.data)\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        parameter.data -= alpha * parameter.grad\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Iteration {i}\")\n",
    "    simple_gradient_descent_iteration(perceptron, X, Y, 1)\n",
    "    print(\"y_pred =\", *[perceptron(x).data for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
