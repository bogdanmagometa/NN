{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nn.value import Value\n",
    "from nn.models import Perceptron\n",
    "from nn.optimizer import AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_perceptron_value(data: List[Value], weights: List[Value], bias: Value) -> Value:\n",
    "    \"\"\"Create and return a perceptron\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list of Value objects (len should be >= 1)\n",
    "        each entry represents a feature of x\n",
    "    weights: list of Value objects (len should be >= 1)\n",
    "        list of weights\n",
    "    bias: Value\n",
    "        represents a bias term\n",
    "    \"\"\"\n",
    "    summands = map(lambda d, w: d * w, data, weights)\n",
    "    \n",
    "    linear_comb = next(summands)\n",
    "    for summand in summands:\n",
    "        linear_comb = linear_comb + summand\n",
    "\n",
    "    logit = linear_comb + bias\n",
    "    \n",
    "    probability = logit.sigmoid()\n",
    "\n",
    "    return probability\n",
    "\n",
    "def make_binary_crossentropy_loss(prediction_values: List[Value], ground_truth: List[float]):\n",
    "    loss = 0\n",
    "    for p, g in zip(prediction_values, ground_truth):\n",
    "        if np.isclose(g, 0):\n",
    "            example_cost = (1 - p).log()\n",
    "        elif np.isclose(g, 1):\n",
    "            example_cost = p.log()\n",
    "        loss = loss + example_cost\n",
    "    loss = (-1) * loss\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example that demonstrates how to creates loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [Value(0), Value(1)]\n",
    "bias = Value(2)\n",
    "\n",
    "data1 = [Value(3), Value(4)]\n",
    "perceptron1 = make_perceptron_value(data1, weights, bias)\n",
    "\n",
    "data2 = [Value(0), Value(0)]\n",
    "perceptron2 = make_perceptron_value(data2, weights, bias)\n",
    "\n",
    "loss = make_binary_crossentropy_loss([perceptron1, perceptron2], [1, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to train a perceptron to behave like an AND gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Loss = 1.4031262483018914\n",
      "y_pred = 0.5060963145098841 0.5742552491057735 0.32750498616094864 0.39063498081802217\n",
      "Iteration 1\n",
      "Loss = 1.0644878132489568\n",
      "y_pred = 0.3904820252921102 0.3812956197707211 0.2287006163201641 0.22193441499000968\n",
      "Iteration 2\n",
      "Loss = 0.9555525882147278\n",
      "y_pred = 0.29588224183183 0.27515864055193157 0.1896866109614129 0.17455702248244423\n",
      "Iteration 3\n",
      "Loss = 0.8805956980128911\n",
      "y_pred = 0.22619109822132777 0.22961847902527815 0.1772472236144697 0.1801055930470825\n",
      "Iteration 4\n",
      "Loss = 0.8075304756561665\n",
      "y_pred = 0.17844149234726883 0.21988184942758937 0.18337606779600776 0.225647653011779\n",
      "Iteration 5\n",
      "Loss = 0.7134377334253097\n",
      "y_pred = 0.14744288341323125 0.23609711600376232 0.2074285012336721 0.3186692556712497\n",
      "Iteration 6\n",
      "Loss = 0.5847194183461181\n",
      "y_pred = 0.1277064643220684 0.27491006629317266 0.25049504104059167 0.4639541705603528\n",
      "Iteration 7\n",
      "Loss = 0.44197738242452733\n",
      "y_pred = 0.11322338315385522 0.3292261364573901 0.30795155588668893 0.6310746218406588\n",
      "Iteration 8\n",
      "Loss = 0.35214947884251013\n",
      "y_pred = 0.09848848463056552 0.37692954815853513 0.3606407260210026 0.7574863616655308\n",
      "Iteration 9\n",
      "Loss = 0.34065046392300014\n",
      "y_pred = 0.08173435386722819 0.3924776700196544 0.38582280613676323 0.8201254674440331\n",
      "Iteration 10\n",
      "Loss = 0.3419333112636239\n",
      "y_pred = 0.0645557530562254 0.37051435183685405 0.37675170503590694 0.83755164755497\n",
      "Iteration 11\n",
      "Loss = 0.3097796446293081\n",
      "y_pred = 0.04902271228144737 0.3214595782329142 0.3402603594258157 0.8257786355601124\n",
      "Iteration 12\n",
      "Loss = 0.25186968278099403\n",
      "y_pred = 0.03634687621149463 0.2614951490291955 0.28920015641975555 0.7925128570504528\n",
      "Iteration 13\n",
      "Loss = 0.1963884533387118\n",
      "y_pred = 0.02680355223328742 0.2059043280040099 0.23739534917670174 0.7455933488907465\n",
      "Iteration 14\n",
      "Loss = 0.16419431864245904\n",
      "y_pred = 0.020045467340979603 0.1638205005229947 0.195005575083754 0.6988071238458546\n",
      "Iteration 15\n",
      "Loss = 0.15598330011227474\n",
      "y_pred = 0.015452178478261601 0.13717346463267444 0.16613112417804432 0.6686678962384413\n",
      "Iteration 16\n",
      "Loss = 0.15643584262277677\n",
      "y_pred = 0.012387948843195119 0.12370492000824594 0.1501439576742524 0.6653637081480206\n",
      "Iteration 17\n",
      "Loss = 0.149981024361319\n",
      "y_pred = 0.010333541562978967 0.1202771175870591 0.1444945912628907 0.6886282440126975\n",
      "Iteration 18\n",
      "Loss = 0.13240442442713113\n",
      "y_pred = 0.008912818790772938 0.12419213551041107 0.14636197487369856 0.7299897652631816\n",
      "Iteration 19\n",
      "Loss = 0.1098304794129944\n",
      "y_pred = 0.007869385227715077 0.1330553601767974 0.1529474546158597 0.7774723441556977\n",
      "Iteration 20\n",
      "Loss = 0.09067713758467019\n",
      "y_pred = 0.00703676193458188 0.14438858679707584 0.1614133336110707 0.8209045338528679\n",
      "Iteration 21\n",
      "Loss = 0.07902703027767803\n",
      "y_pred = 0.006314473071011984 0.15560213413138324 0.16906213512064058 0.8550741927067745\n",
      "Iteration 22\n",
      "Loss = 0.07383739186754838\n",
      "y_pred = 0.005648964050338342 0.16432366569738086 0.17371628278969414 0.8791812951120732\n",
      "Iteration 23\n",
      "Loss = 0.07180868426013153\n",
      "y_pred = 0.005018318978534353 0.168830414970533 0.17405488151970483 0.8945923142877569\n",
      "Iteration 24\n",
      "Loss = 0.0699347745325422\n",
      "y_pred = 0.004420156568984257 0.16835254269711267 0.1697524199773638 0.9031232000466869\n",
      "Iteration 25\n",
      "Loss = 0.06656311487404303\n",
      "y_pred = 0.0038619230705043417 0.16311754310462737 0.16137691508459814 0.9063184446743977\n",
      "Iteration 26\n",
      "Loss = 0.06144098984873795\n",
      "y_pred = 0.003353485778394115 0.15413278492085344 0.1500963527724999 0.9053385700973139\n",
      "Iteration 27\n",
      "Loss = 0.05525786268115186\n",
      "y_pred = 0.002902405964433631 0.1428133941437928 0.13730708250609056 0.9010862676919482\n",
      "Iteration 28\n",
      "Loss = 0.04904125085269598\n",
      "y_pred = 0.0025119104601367234 0.13060467863751177 0.1243093053071504 0.8943847828365304\n",
      "Iteration 29\n",
      "Loss = 0.043671269258600626\n",
      "y_pred = 0.0021809207775186157 0.11871400390832132 0.11210955186941263 0.8861281670561958\n",
      "Iteration 30\n",
      "Loss = 0.039633117097724914\n",
      "y_pred = 0.0019051666718591174 0.1079916229468762 0.10136045348731045 0.8773593301956275\n",
      "Iteration 31\n",
      "Loss = 0.03698049570796619\n",
      "y_pred = 0.0016785411113044806 0.09893298197677933 0.09239689067269619 0.8692449778305712\n",
      "Iteration 32\n",
      "Loss = 0.03542461365159007\n",
      "y_pred = 0.0014942179894997568 0.09174660326478526 0.08531430759681741 0.8629399179125871\n",
      "Iteration 33\n",
      "Loss = 0.0344836690805588\n",
      "y_pred = 0.0013453924164526958 0.08643848392143251 0.08004915373724127 0.8593771206201889\n",
      "Iteration 34\n",
      "Loss = 0.033656082802507384\n",
      "y_pred = 0.0012256964800785614 0.08288504976401141 0.0764424493775902 0.8590650881838996\n",
      "Iteration 35\n",
      "Loss = 0.03257753124170136\n",
      "y_pred = 0.0011294008596987395 0.0808853152042842 0.07428274726722163 0.8619842533625738\n",
      "Iteration 36\n",
      "Loss = 0.031109982623450445\n",
      "y_pred = 0.0010514957068073513 0.08019371512837739 0.0733321878384462 0.8676313568042076\n",
      "Iteration 37\n",
      "Loss = 0.029331205063980945\n",
      "y_pred = 0.0009877071132579017 0.08053950136872291 0.07334151621957927 0.8751889249139758\n",
      "Iteration 38\n",
      "Loss = 0.027444369311580074\n",
      "y_pred = 0.0009344763546401429 0.08163916948894034 0.07405944413450087 0.8837433189118158\n",
      "Iteration 39\n",
      "Loss = 0.02566624440405233\n",
      "y_pred = 0.0008889144538922726 0.08320690425247454 0.07524012739555441 0.892469647557245\n",
      "Iteration 40\n",
      "Loss = 0.024148032551149168\n",
      "y_pred = 0.0008487403451612993 0.0849657972992513 0.07665072213802407 0.900737583305339\n",
      "Iteration 41\n",
      "Loss = 0.022948267643216044\n",
      "y_pred = 0.0008122104823434433 0.08666051400701431 0.07807953732516727 0.9081371124358428\n",
      "Iteration 42\n",
      "Loss = 0.022045908634364723\n",
      "y_pred = 0.0007780466729300486 0.08807058015016912 0.07934436433873078 0.9144508766073709\n",
      "Iteration 43\n",
      "Loss = 0.021371213110177152\n",
      "y_pred = 0.0007453662542696996 0.08902256118044462 0.08030003221522194 0.9196043482553662\n",
      "Iteration 44\n",
      "Loss = 0.020837127963189137\n",
      "y_pred = 0.0007136157749611813 0.08939908497549784 0.08084401502636951 0.923616020111175\n",
      "Iteration 45\n",
      "Loss = 0.02036297279117083\n",
      "y_pred = 0.0006825076070765235 0.0891429282794552 0.08091901070492614 0.9265582070759821\n",
      "Iteration 46\n",
      "Loss = 0.019888510720228084\n",
      "y_pred = 0.0006519588775926695 0.08825520822660697 0.080511829733149 0.9285307651661157\n",
      "Iteration 47\n",
      "Loss = 0.01937941308422031\n",
      "y_pred = 0.000622033212002827 0.08678788093586128 0.07964858941668096 0.9296458811916392\n",
      "Iteration 48\n",
      "Loss = 0.01882612303202204\n",
      "y_pred = 0.0005928870426745642 0.08483188630315912 0.07838690815929988 0.9300207897951663\n",
      "Iteration 49\n",
      "Loss = 0.0182383976804643\n",
      "y_pred = 0.0005647228533701761 0.08250302813374728 0.07680631093784437 0.9297753793455074\n",
      "Iteration 50\n",
      "Loss = 0.01763777530908724\n",
      "y_pred = 0.0005377514263244795 0.07992784095825603 0.07499824546166207 0.9290322210506415\n",
      "Iteration 51\n",
      "Loss = 0.017049911408177507\n",
      "y_pred = 0.0005121641396112025 0.07723130304420052 0.07305695479347482 0.9279171681612733\n",
      "Iteration 52\n",
      "Loss = 0.01649818977159701\n",
      "y_pred = 0.00048811509623163465 0.07452752682266137 0.07107206082331206 0.9265592057020087\n",
      "Iteration 53\n",
      "Loss = 0.015999378607442123\n",
      "y_pred = 0.0004657117999043613 0.07191377062810786 0.06912324115627376 0.925088708100867\n",
      "Iteration 54\n",
      "Loss = 0.015561531415378173\n",
      "y_pred = 0.00044501248174850627 0.0694674882972963 0.06727696915141258 0.923633738956423\n",
      "Iteration 55\n",
      "Loss = 0.015183926370419779\n",
      "y_pred = 0.00042602805923596185 0.06724576776348926 0.06558501237293184 0.9223145332209606\n",
      "Iteration 56\n",
      "Loss = 0.014858600378643248\n",
      "y_pred = 0.0004087269503012776 0.06528639847126125 0.0640842626739716 0.9212368077268978\n",
      "Iteration 57\n",
      "Loss = 0.014572914062584468\n",
      "y_pred = 0.0003930413898357158 0.06360987754130545 0.06279747053082678 0.9204849605272084\n",
      "Iteration 58\n",
      "Loss = 0.014312534809783662\n",
      "y_pred = 0.0003788743413423629 0.06222182740337719 0.06173452821420631 0.9201164218316408\n",
      "Iteration 59\n",
      "Loss = 0.014064237385993182\n",
      "y_pred = 0.0003661064605226921 0.061115480485419794 0.06089404530131765 0.9201583154619585\n",
      "Iteration 60\n",
      "Loss = 0.013818015331935229\n",
      "y_pred = 0.0003546028110066556 0.06027404485182904 0.06026505400635892 0.9206071723637111\n",
      "Iteration 61\n",
      "Loss = 0.0135681840404287\n",
      "y_pred = 0.00034421916461018377 0.05967287942153267 0.05982875425217493 0.9214318117965029\n",
      "Iteration 62\n",
      "Loss = 0.013313411058237341\n",
      "y_pred = 0.00033480777407605885 0.059281476975968854 0.05956025472708367 0.9225788585268594\n",
      "Iteration 63\n",
      "Loss = 0.013055862698857053\n",
      "y_pred = 0.00032622252428469637 0.05906528505731847 0.05943028908988514 0.9239798895419404\n",
      "Iteration 64\n",
      "Loss = 0.012799830775400537\n",
      "y_pred = 0.000318323378002406 0.058987399636029066 0.05940689203900187 0.9255590222292083\n",
      "Iteration 65\n",
      "Loss = 0.012550252638798717\n",
      "y_pred = 0.0003109800493446127 0.05901015446443408 0.059457014846307235 0.9272398745993004\n",
      "Iteration 66\n",
      "Loss = 0.012311467501266996\n",
      "y_pred = 0.00030407486463250724 0.059096609089003486 0.05954805013631225 0.928951155854746\n",
      "Iteration 67\n",
      "Loss = 0.012086410196755146\n",
      "y_pred = 0.00029750480163973477 0.05921191738662265 0.05964922588203148 0.9306305478071615\n",
      "Iteration 68\n",
      "Loss = 0.01187629071556735\n",
      "y_pred = 0.0002911827285824939 0.05932454122750282 0.05973282221499541 0.9322268948087669\n",
      "Iteration 69\n",
      "Loss = 0.011680689816265308\n",
      "y_pred = 0.0002850378900755807 0.0594072638429901 0.059775163944203576 0.9337009626702351\n",
      "Iteration 70\n",
      "Loss = 0.011497936819319399\n",
      "y_pred = 0.00027901570807669236 0.059437956557004734 0.05975734773954946 0.9350251407722089\n",
      "Iteration 71\n",
      "Loss = 0.011325621469974446\n",
      "y_pred = 0.00027307698275479456 0.05940006102921568 0.05966567566720263 0.9361824696799224\n",
      "Iteration 72\n",
      "Loss = 0.011161111850290903\n",
      "y_pred = 0.0002671965925026206 0.05928276577376138 0.059491784867144834 0.9371653183212536\n",
      "Iteration 73\n",
      "Loss = 0.01100198740015373\n",
      "y_pred = 0.0002613618041501979 0.05908087775121761 0.0592324842692126 0.9379739479327633\n",
      "Iteration 74\n",
      "Loss = 0.01084633675359706\n",
      "y_pred = 0.00025557031273422593 0.058794413605613584 0.05888933031129658 0.9386151119474698\n",
      "Iteration 75\n",
      "Loss = 0.010692906093147367\n",
      "y_pred = 0.0002498281331636602 0.05842795670365017 0.05846799153672853 0.9391007673280827\n",
      "Iteration 76\n",
      "Loss = 0.01054111111302702\n",
      "y_pred = 0.0002441474622585729 0.05798984216227255 0.0579774641779039 0.9394469195490696\n",
      "Iteration 77\n",
      "Loss = 0.010390943306585595\n",
      "y_pred = 0.0002385446184021236 0.0574912403898549 0.05742920595332016 0.9396725904793973\n",
      "Iteration 78\n",
      "Loss = 0.010242809660994417\n",
      "y_pred = 0.00023303814825415646 0.056945209755790156 0.05683625321630269 0.9397988827743846\n",
      "Iteration 79\n",
      "Loss = 0.010097345415789443\n",
      "y_pred = 0.00022764716763295072 0.05636578178961729 0.056212378409569594 0.939848111963427\n",
      "Iteration 80\n",
      "Loss = 0.009955234300812745\n",
      "y_pred = 0.00022238997944861398 0.05576712983446189 0.05557133244239201 0.9398429841051115\n",
      "Iteration 81\n",
      "Loss = 0.00981706177807741\n",
      "y_pred = 0.00021728298816336568 0.055162856881087755 0.054926202374116447 0.9398058089059103\n",
      "Iteration 82\n",
      "Loss = 0.009683216339894505\n",
      "y_pred = 0.00021233990983833673 0.05456542284061622 0.054288900748646625 0.9397577522149717\n",
      "Iteration 83\n",
      "Loss = 0.009553843620701703\n",
      "y_pred = 0.0002075712607492418 0.05398571770408338 0.0536697906593518 0.939718144890808\n",
      "Iteration 84\n",
      "Loss = 0.009428849286677539\n",
      "y_pred = 0.00020298409627357873 0.05343277607380527 0.053077441053226176 0.9397038747585353\n",
      "Iteration 85\n",
      "Loss = 0.009307940229389848\n",
      "y_pred = 0.00019858196499398804 0.0529136208788038 0.05251850019107526 0.9397288929783498\n",
      "Iteration 86\n",
      "Loss = 0.009190689913237784\n",
      "y_pred = 0.00019436503997722753 0.05243321954807326 0.05199767135589678 0.939803864746445\n",
      "Iteration 87\n",
      "Loss = 0.009076612815845358\n",
      "y_pred = 0.00019033038903134344 0.05199453396966586 0.05151777332921245 0.9399359870045523\n",
      "Iteration 88\n",
      "Loss = 0.008965234414297153\n",
      "y_pred = 0.0001864723474981918 0.05159864549947681 0.051079868199926104 0.9401289839380802\n",
      "Iteration 89\n",
      "Loss = 0.008856146488925541\n",
      "y_pred = 0.00018278296005218422 0.05124493739354645 0.050683440120181 0.9403832765196202\n",
      "Iteration 90\n",
      "Loss = 0.008749041839031024\n",
      "y_pred = 0.0001792524615116212 0.050931318728516126 0.05032661017599576 0.9406963076561973\n",
      "Iteration 91\n",
      "Loss = 0.008643726976285702\n",
      "y_pred = 0.00017586977048878515 0.0506544757195503 0.05000637424259022 0.9410629919995365\n",
      "Iteration 92\n",
      "Loss = 0.008540115217535374\n",
      "y_pred = 0.0001726229736206634 0.050410138087832373 0.04971885233586684 0.9414762510001778\n",
      "Iteration 93\n",
      "Loss = 0.008438205295315266\n",
      "y_pred = 0.000169499782034657 0.05019334966169697 0.04945953946306406 0.9419275902769726\n",
      "Iteration 94\n",
      "Loss = 0.008338051895375044\n",
      "y_pred = 0.00016648794555333338 0.04999873371538174 0.049223549312438136 0.9424076778179333\n",
      "Iteration 95\n",
      "Loss = 0.008239734472614663\n",
      "y_pred = 0.00016357561387740925 0.04982074472167379 0.049005843348208455 0.9429068870847165\n",
      "Iteration 96\n",
      "Loss = 0.008143329586230102\n",
      "y_pred = 0.00016075163754917146 0.04965389931126346 0.04880143905554067 0.9434157773903645\n",
      "Iteration 97\n",
      "Loss = 0.008048890260130497\n",
      "y_pred = 0.00015800580482533064 0.04949298038129818 0.04860559226904207 0.9439254934144107\n",
      "Iteration 98\n",
      "Loss = 0.007956433961499587\n",
      "y_pred = 0.0001553290136139346 0.04933320954310411 0.04841394975561106 0.9444280750101961\n",
      "Iteration 99\n",
      "Loss = 0.007865939068937513\n",
      "y_pred = 0.00015271338029552054 0.049170384472549074 0.048222669520002745 0.9449166765001764\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    ]\n",
    "\n",
    "Y = [0, 0, 0, 1]\n",
    "\n",
    "perceptron = Perceptron(no_weights=2)\n",
    "adam_optimizer = AdamOptimizer(perceptron.parameters(), lr=0.5)\n",
    "\n",
    "def simple_gradient_descent_iteration(model, X, Y, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = sum((model(x) - y)**2 for x, y in zip(X, Y))\n",
    "\n",
    "    loss.back_prop()\n",
    "    print(\"Loss =\", loss.data)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Iteration {i}\")\n",
    "    simple_gradient_descent_iteration(perceptron, X, Y, adam_optimizer)\n",
    "    print(\"y_pred =\", *[perceptron(x).data for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
